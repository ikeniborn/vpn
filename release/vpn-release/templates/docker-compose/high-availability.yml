# High Availability Override Configuration
# Use with: docker-compose -f base.yml -f production.yml -f high-availability.yml up
version: '3.8'

services:
  # VPN Server - Scale to multiple instances
  vpn-server:
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    environment:
      - INSTANCE_ID={{.Task.Name}}
      - SERVICE_NAME=vpn-server
      - ENABLE_METRICS=true
      - METRICS_PORT=9100
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:8080/health || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Load Balancer for VPN Servers
  vpn-lb:
    image: haproxy:2.8-alpine
    container_name: vpn-lb
    restart: unless-stopped
    ports:
      - "${VPN_LB_PORT:-8444}:8443"
      - "${VPN_LB_STATS_PORT:-8404}:8404"
    volumes:
      - ./configs/haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - haproxy-socket:/var/run/haproxy
    networks:
      vpn-network:
        ipv4_address: 172.20.0.5
    depends_on:
      - vpn-server
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'
    healthcheck:
      test: ["CMD-SHELL", "haproxy -c -f /usr/local/etc/haproxy/haproxy.cfg || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3

  # Nginx - Enhanced for HA
  nginx-proxy:
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    volumes:
      - ./configs/nginx/ha:/etc/nginx/conf.d:ro
      - nginx-cache:/var/cache/nginx
      - nginx-certs:/etc/nginx/certs:ro
    environment:
      - NGINX_UPSTREAM_CHECK_INTERVAL=5s
      - NGINX_UPSTREAM_MAX_FAILS=3
      - NGINX_UPSTREAM_FAIL_TIMEOUT=10s

  # API Service - Scale for HA
  vpn-api:
    deploy:
      replicas: 3
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      update_config:
        parallelism: 1
        delay: 20s
        failure_action: rollback
        monitor: 30s
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    environment:
      - CLUSTER_MODE=true
      - INSTANCE_ID={{.Task.Name}}
      - SESSION_STORE=redis
      - CACHE_STORE=redis

  # PostgreSQL - Primary/Replica Setup
  postgres-primary:
    image: postgres:15-alpine
    container_name: postgres-primary
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-vpndb}
      - POSTGRES_USER=${POSTGRES_USER:-vpnuser}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-changepassword}
      - POSTGRES_REPLICATION_MODE=master
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${REPLICATION_PASSWORD:-replicationpass}
      - POSTGRES_MAX_CONNECTIONS=200
      - POSTGRES_SHARED_BUFFERS=512MB
    volumes:
      - postgres-primary-data:/var/lib/postgresql/data
      - ./configs/postgres/primary:/docker-entrypoint-initdb.d:ro
    networks:
      vpn-internal:
        ipv4_address: 172.21.0.30
    deploy:
      placement:
        constraints:
          - node.labels.postgres == primary
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    command: >
      postgres
      -c wal_level=replica
      -c hot_standby=on
      -c max_wal_senders=10
      -c max_replication_slots=10
      -c hot_standby_feedback=on
      -c wal_log_hints=on
      -c archive_mode=on
      -c archive_command='test ! -f /mnt/archive/%f && cp %p /mnt/archive/%f'

  postgres-replica:
    image: postgres:15-alpine
    restart: unless-stopped
    environment:
      - POSTGRES_REPLICATION_MODE=slave
      - POSTGRES_MASTER_HOST=postgres-primary
      - POSTGRES_MASTER_PORT=5432
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${REPLICATION_PASSWORD:-replicationpass}
    volumes:
      - postgres-replica-data:/var/lib/postgresql/data
    networks:
      vpn-internal:
    depends_on:
      - postgres-primary
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.labels.postgres != primary
        preferences:
          - spread: node.labels.zone
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'

  # Redis Sentinel for HA
  redis-master:
    image: redis:7-alpine
    container_name: redis-master
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --requirepass ${REDIS_PASSWORD:-changepassword}
      --masterauth ${REDIS_PASSWORD:-changepassword}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
    volumes:
      - redis-master-data:/data
    networks:
      vpn-internal:
        ipv4_address: 172.21.0.40
    deploy:
      placement:
        constraints:
          - node.labels.redis == master

  redis-replica:
    image: redis:7-alpine
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --requirepass ${REDIS_PASSWORD:-changepassword}
      --masterauth ${REDIS_PASSWORD:-changepassword}
      --replicaof redis-master 6379
    volumes:
      - redis-replica-data:/data
    networks:
      vpn-internal:
    depends_on:
      - redis-master
    deploy:
      replicas: 2
      placement:
        preferences:
          - spread: node.labels.zone

  redis-sentinel:
    image: redis:7-alpine
    restart: unless-stopped
    command: >
      redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./configs/redis/sentinel.conf:/etc/redis/sentinel.conf:ro
    networks:
      vpn-internal:
    depends_on:
      - redis-master
      - redis-replica
    deploy:
      replicas: 3
      placement:
        constraints:
          - node.role == manager

  # Consul for Service Discovery
  consul:
    image: consul:latest
    container_name: consul
    restart: unless-stopped
    ports:
      - "${CONSUL_PORT:-8500}:8500"
      - "${CONSUL_DNS_PORT:-8600}:8600/udp"
    environment:
      - CONSUL_BIND_INTERFACE=eth0
      - CONSUL_CLIENT_INTERFACE=eth0
    volumes:
      - consul-data:/consul/data
      - ./configs/consul:/consul/config:ro
    networks:
      vpn-network:
        ipv4_address: 172.20.0.6
      vpn-internal:
        ipv4_address: 172.21.0.6
    command: agent -server -ui -node=consul-1 -bootstrap-expect=1 -client=0.0.0.0
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          memory: 256M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.25'

  # Keepalived for Virtual IP
  keepalived:
    image: osixia/keepalived:latest
    container_name: keepalived
    restart: unless-stopped
    cap_add:
      - NET_ADMIN
      - NET_BROADCAST
      - NET_RAW
    environment:
      - KEEPALIVED_INTERFACE=eth0
      - KEEPALIVED_VIRTUAL_IPS=${VIRTUAL_IP:-172.20.0.100}
      - KEEPALIVED_PRIORITY=100
      - KEEPALIVED_ROUTER_ID=51
    volumes:
      - ./configs/keepalived:/container/service/keepalived/assets/custom:ro
    networks:
      vpn-network:
    deploy:
      mode: global
      placement:
        constraints:
          - node.role == manager

  # Monitoring for HA
  prometheus:
    deploy:
      replicas: 2
      placement:
        constraints:
          - node.role == manager
        preferences:
          - spread: node.labels.zone
    volumes:
      - ./configs/prometheus/ha:/etc/prometheus:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus-ha.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--storage.tsdb.min-block-duration=2h'
      - '--storage.tsdb.max-block-duration=2h'

  grafana:
    deploy:
      replicas: 2
      placement:
        preferences:
          - spread: node.labels.zone
    environment:
      - GF_DATABASE_TYPE=postgres
      - GF_DATABASE_HOST=postgres-primary:5432
      - GF_DATABASE_NAME=grafana
      - GF_DATABASE_USER=${POSTGRES_USER:-vpnuser}
      - GF_DATABASE_PASSWORD=${POSTGRES_PASSWORD:-changepassword}
      - GF_SESSION_PROVIDER=redis
      - GF_SESSION_PROVIDER_CONFIG=addr=redis-master:6379,password=${REDIS_PASSWORD:-changepassword}
      - GF_HA_ENABLED=true

# Additional volumes for HA
volumes:
  haproxy-socket:
    driver: local
  nginx-cache:
    driver: local
  postgres-primary-data:
    driver: local
  postgres-replica-data:
    driver: local
  redis-master-data:
    driver: local
  redis-replica-data:
    driver: local
  consul-data:
    driver: local

# Networks remain the same as base configuration